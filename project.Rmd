---
title: "Project Group 1"
author: "Adil Iqbal, Angelita Krepel"
date: "11/22/2022"
output: word_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Project 1

### Introduction

This data was extracted from the U.S. Census in 1994. Our goal will be to predict whether an individual earns more than $50,000 USD per year (`class`) based on data collected by the census alone. Examples of data collected by the census include education level (`education`), marital status (`marital.status`), and country of origin (`native.country`) among others. All features are either categorical or continuous integer values.


#### Features

age: continuous.
workclass: Private, Self-emp-not-inc, Self-emp-inc, Federal-gov, Local-gov, State-gov, Without-pay, Never-worked.
fnlwgt: continuous.
education: Bachelors, Some-college, 11th, HS-grad, Prof-school, Assoc-acdm, Assoc-voc, 9th, 7th-8th, 12th, Masters, 1st-4th, 10th, Doctorate, 5th-6th, Preschool.
education.num: continuous.
marital.status: Married-civ-spouse, Divorced, Never-married, Separated, Widowed, Married-spouse-absent, Married-AF-spouse.
occupation: Tech-support, Craft-repair, Other-service, Sales, Exec-managerial, Prof-specialty, Handl sdasdasders-cleaners, Machine-op-inspct, Adm-clerical, Farming-fishing, Transport-moving, Priv-house-serv, Protective-serv, Armed-Forces.
relationship: Wife, Own-child, Husband, Not-in-family, Other-relative, Unmarried.
race: White, Asian-Pac-Islander, Amer-Indian-Eskimo, Other, Black.
sex: Female, Male.
capital.gain: continuous.
capital.loss: continuous.
hours.per.week: continuous.
native.country: United-States, Cambodia, England, Puerto-Rico, Canada, Germany, Outlying-US(Guam-USVI-etc), India, Japan, Greece, South, China, Cuba, Iran, Honduras, Philippines, Italy, Poland, Jamaica, Vietnam, Mexico, Portugal, Ireland, France, Dominican-Republic, Laos, Ecuador, Taiwan, Haiti, Columbia, Hungary, Guatemala, Nicaragua, Scotland, Thailand, Yugoslavia, El-Salvador, Trinadad&Tobago, Peru, Hong, Holand-Netherlands.

#### Response Variable

class: >50K, <=50K.

#### Question
Can we predict if an individual will make 50K or more based on a list of predictors?

```{r}
#Dosbol
set.seed(1)
data = read.csv("adult.csv", stringsAsFactors = T)

summary(data)
```

#### Removing missing values. 

In this data set, missing values are marked with a "?" character. Since all the missing data occurs in our categorical variables, it does not make sense to replace them with a median or a mean. So we are opting to remove them entirely for our analysis. After removing the observations with missing variables, we are left with 45,222 observations.

```{r}
#Adil
cols_missing_vals <- c("occupation", "workclass", "native.country")

for (column in cols_missing_vals) {
  data <- data[!grepl("\\?", data[, column]),]
}

summary(data)
```

#### Set categorical features to dummy variables
In this data, many categorical variables are needed to be converted to numeric and make a Dummy variable for the response variable.


```{r, results='hide'}
#Dosbol
### 1. Class  (<=50K or 50K>)
data$class = ifelse(data$class==" <=50K",0,1) # Our Response Variable. I made 0 for below 50K and 1 for above 50K
data$class = as.factor(data$class)

### 2. Sex  (Female, Male)
#data$sex = ifelse(data$sex==" Female",0,1)
#data$sex = as.factor(data$sex)


### 3. Workclass
#data$workclass = ifelse(data$workclass==" Private",0,
#                                  ifelse(data$workclass==" State-gov",1,
#                                          ifelse(data$workclass ==" Federal-gov",2,
#                                                ifelse(data$workclass == " Self-emp-not-inc",3, 
#                                                       ifelse(data$workclass==" Self-emp-inc",4, 
#                                                              ifelse(data$workclass==" Local-gov",5,6)))))) #
#data$workclass = as.factor(data$workclass)


### 4. Marital Status
#data$marital.status = ifelse(data$marital.status == " Widowed",0,
#                                   ifelse(data$marital.status == " Divorced",1,
#                                          ifelse(data$marital.status == " Separated",2,
#                                                 ifelse(data$marital.status == " Never-married",3,
#                                                        ifelse(data$marital.status== " Married-civ-spouse",4,
#                                                               ifelse(data$marital.status==" Married-AF-spouse",4,5
#                                                                      ))))))
#data$marital.status = as.factor(data$marital.status)



### 5. Race
#data$race = ifelse(data$race ==" White",0,
#                         ifelse(data$race == " Black",1,
#                                ifelse(data$race==" Asian-Pac_islander",2,
#                                       ifelse(data$race==" Amer-Indian-Eskimo",3,4))))
#data$race = as.factor(data$race)

### 6. Relationship
#data$relationship = ifelse(data$relationship==" Husband",1,
#                                 ifelse(data$relationship==" Wife",1,0))
#data$relationship = as.factor(data$relationship)


### 7. Education
# I grouped them by following:

# 0 means all students who are in 1st to 12 grade
# 1 means all students who are in HS-grad, Assos-acdm, Prof-school
# 2 means all students who are in Bachelors Degree
# 3 means all students who are in Masters' Degree
# 4 means all students who are in Doc 
# and 5 means UNKNOWNs
#data$education=ifelse(data$education==" Prepschool",0,
#                            ifelse(data$education==" 1st-4th",0,
#                                   ifelse(data$education== "5th-6th",0,
#                                          ifelse(data$education==" 7th-8th",0,
#                                                 ifelse(data$education==" 9th",0, ifelse(data$education==" 10th",0,
                                                                                                #ifelse(data$education==" 11th",0,
                                                                                                       #ifelse(data$education==" 12th",0,
                                                                                                              #ifelse(data$education==" HS-grad",1,
                                                                                                                  #   ifelse(data$education==" Assos-voc", 1,
                                                                                                                  #          ifelse(data$education==" Assoc-acdm",1,
                                                                                                                  #                 ifelse(data$education==" Prof-school",1,
                                                                                                                  #                        ifelse(data$education==" Some-college",1,
                                                                                                                  #                               ifelse(data$education==" Bachelors",2,
                                                                                                                  #                                      ifelse(data$education==" Masters",3,
#                                                                                                                                                               ifelse(data$education==" Doctorate",4, 5))))))))))))))))

#print(data$education_dummy)
#data$education = as.factor(data$education)


### 8. Native Country

data$native.country = ifelse(data$native.country==" United-States",0,1)
# So most people are from the United States, so I assigned them to 0 and other countries to 1
data$native.country = as.factor(data$native.country)

# Omit NA rows
data = na.omit(data)
```



```{r, results='hide'}
#Angelita
#turning occupation into dummy variables
#install.packages('fastDummies')
#library('fastDummies')
#data <- dummy_cols(data, select_columns = 'occupation')

#as.factor(data$occupation)
```
```{r}
#drop original occupation column to keep new dummies
#data <- data[-c(7)]
```


#### Split data into training and testing set

Below, we are now splitting the data into training and testing sets. With the training set containing 80% of the data and the testing set containing 20%. 

```{r}
#Adil
sample <- sample(c(TRUE, FALSE), nrow(data), replace=TRUE, prob=c(0.8,0.2))
train  <- data[sample, ]
test   <- data[!sample, ]

print(nrow(train))
print(nrow(test))
```


```{r}
#checking correlations
#pairs(adult[sapply(adult, is.numeric)])
#cor(adult[sapply(adult, is.numeric)])
```
Doesn't appear that from the original model there is much correlation between the data, but after making the occupation dummies there may be some correlation between those.

####Methods

#### Model 1: Logistic regression

We will be using logistic regression, because the response variable is a categorical variable that takes 1 if their salary is >=50K. The advantages of using logistic regression is it is easy to implement and interpret, makes no assumptions about classes within the predictors, provides a measure of which predictors are most important in the predictions, less likely to overfit. The disadvantages of logistic regression is non-linear problems cannot be used with this, requires little to no multicollinearity between independent variables, with high dimensional datasets you can overfit/lead to less accurate results. 

The model formula for logistic regression is: $$\frac{e^{b_0 + b_1 x}}{1+e^{b_0 + b_1 x}}$$

```{r, warning=FALSE}
#Angelita
lr.glm = glm(class ~., data = train, family = "binomial")
glm.pred = predict.glm(lr.glm, test, type = "response")
yHat = glm.pred > 0.5
table(test$class,yHat)
```
```{r}
(1477+1664)/(3068+1477+1664+2783)
```
The test error rate is kind of large sitting at about 35%. This means our model only predicted about 65% of the data correctly. Seeing this error rate, there may be some predictors that are less important/not needed in predicting the response, therefore there could be some predictors we can leave out of the model. So, the next step is to look at the summary to see which of those varibles are significant.  

```{r}
#Angelita
summary(lr.glm)
```
Based on the summary of the logistic regression model we can see that not all the variables are important in predicting, so we will try again but this time with the variables deemed important.

```{r}
#Angelita 
lr.glm2 = glm(class ~  age+education+occupation+relationship+sex+capital.gain+capital.loss+hours.per.week, data = train, family = "binomial")
glm.pred2 = predict.glm(lr.glm2, test, type = "response")
yHat2 = glm.pred2 > 0.5
table(test$class,yHat2)
summary(lr.glm2)
```
```{r}
(1482+1648)/(3081+1482+1648+2799)
```
The results when removing the less important variables come to be about the same as when leaving them in there. 




#### Decision Tree Model

We will be using decision tree as our second model, because the dimensions of our dataset are pretty large and this should be able to manage it better than the logistic regression did. We are also using it, because our goal is to classify someone 1 if their salary is >=50K or 0 if below 50K. Some advantages of decision trees are: it is easy to understand and interpret, it takes less data preparation, is a non-parametric algorithm therefore needs little to no assumptions for classification, and you can use it for non-linear problems. Some disadvantages are:it can be prone to overfitting the data, it cannot handle too many features so you have to do feature reduction, can have high variances and changes to the data can have a large affect in the predictions.  


Here we create the tree and plot it. 
```{r}
#UD
library(tree)
plot.new()
class.tree = tree(class ~ ., data=train)
summary(class.tree)
plot(class.tree)
text(class.tree)
```

Pruning/Cross Validating tree for optimization


Here we are pruning the tree to obtain better results. 
#### Cross Validation of tree
```{r}
#UD
class.cv = cv.tree(class.tree,FUN = prune.misclass)
class.cv
plot(class.cv$size,class.cv$dev,type = "b")
class.prune = prune.misclass(class.tree,best = 3)
summary(class.prune)
plot(class.prune)
text(class.prune)
```

####Conclusion 



####Bibliography



